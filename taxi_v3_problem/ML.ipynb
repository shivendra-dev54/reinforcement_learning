{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"R62Z3eNulw5C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744473387297,"user_tz":-330,"elapsed":6740,"user":{"displayName":"Shivendra Devadhe","userId":"17542011610081090060"}},"outputId":"e4985a94-4e49-4f5d-fbf9-34614eb7a079"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.4)\n","Requirement already satisfied: gym==0.26.2 in /usr/local/lib/python3.11/dist-packages (0.26.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (2.2.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (3.1.1)\n","Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2) (0.0.8)\n"]}],"source":["!pip install numpy --upgrade\n","!pip install gym==0.26.2"]},{"cell_type":"code","source":["import gymnasium as gym\n","import numpy as np\n","import random\n","import time\n","\n","# Create the Taxi-v3 environment without rendering during training\n","env = gym.make(\"Taxi-v3\", render_mode=None)  # No rendering during training\n","\n","# Initialize Q-table\n","state_space = env.observation_space.n\n","action_space = env.action_space.n\n","q_table = np.zeros((state_space, action_space))\n","\n","# Hyperparameters\n","alpha = 0.7        # Learning rate\n","gamma = 0.618      # Discount factor\n","epsilon = 1.0      # Exploration rate\n","max_epsilon = 1.0\n","min_epsilon = 0.01\n","decay_rate = 0.01\n","episodes = 10000   # Reduced for faster training\n","max_steps = 100\n","\n","# Training - No rendering for speed\n","print(\"Starting training...\")\n","start_time = time.time()\n","\n","for episode in range(episodes):\n","    state, info = env.reset()\n","    done = False\n","\n","    for step in range(max_steps):\n","        # Exploration-exploitation trade-off\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample()  # Explore\n","        else:\n","            action = np.argmax(q_table[state])  # Exploit\n","\n","        next_state, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","\n","        # Q-Learning update\n","        q_table[state, action] = q_table[state, action] + alpha * (\n","            reward + gamma * np.max(q_table[next_state]) - q_table[state, action]\n","        )\n","\n","        state = next_state\n","\n","        if done:\n","            break\n","\n","    # Decay epsilon\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n","\n","    # Print progress every 2000 episodes\n","    if episode % 2000 == 0:\n","        print(f\"Episode {episode}/{episodes}\")\n","\n","training_time = time.time() - start_time\n","print(f\"Training finished in {training_time:.2f} seconds.\\n\")\n","\n","# Evaluate the trained policy\n","total_epochs, total_penalties = 0, 0\n","episodes_to_test = 5\n","\n","# Only use rendering for evaluation\n","eval_env = gym.make(\"Taxi-v3\", render_mode=\"None\")  # Or \"ansi\" for terminal rendering\n","\n","for ep in range(episodes_to_test):\n","    state, info = eval_env.reset()\n","    done = False\n","    steps = 0\n","    penalties = 0\n","\n","    print(f\"\\nEpisode {ep + 1}\")\n","\n","    while not done and steps < 100:\n","        action = np.argmax(q_table[state])\n","        state, reward, terminated, truncated, _ = eval_env.step(action)\n","        done = terminated or truncated\n","\n","        if reward == -10:  # Count penalties (illegal moves)\n","            penalties += 1\n","\n","        steps += 1\n","        time.sleep(0.5)  # Slow down rendering to make it visible\n","\n","    total_penalties += penalties\n","    total_epochs += steps\n","    print(f\"Episode finished in {steps} steps with {penalties} penalties.\")\n","\n","print(f\"\\nResults after {episodes_to_test} episodes:\")\n","print(f\"Average steps per episode: {total_epochs / episodes_to_test}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes_to_test}\")\n","\n","eval_env.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDokLfnqt1XJ","executionInfo":{"status":"ok","timestamp":1744473510159,"user_tz":-330,"elapsed":38824,"user":{"displayName":"Shivendra Devadhe","userId":"17542011610081090060"}},"outputId":"e08ceebd-35db-4e05-cbf6-4df34ed5c217"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training...\n","Episode 0/10000\n","Episode 2000/10000\n","Episode 4000/10000\n","Episode 6000/10000\n","Episode 8000/10000\n","Training finished in 5.82 seconds.\n","\n","\n","Episode 1\n","Episode finished in 13 steps with 0 penalties.\n","\n","Episode 2\n","Episode finished in 12 steps with 0 penalties.\n","\n","Episode 3\n","Episode finished in 10 steps with 0 penalties.\n","\n","Episode 4\n","Episode finished in 15 steps with 0 penalties.\n","\n","Episode 5\n","Episode finished in 11 steps with 0 penalties.\n","\n","Results after 5 episodes:\n","Average steps per episode: 12.2\n","Average penalties per episode: 0.0\n"]}]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"15kHHOGDIlWoomsF6Z2TwRc3BDBIwFLQ0","authorship_tag":"ABX9TyOloNLE9QA78Ibn8HNUI6Tl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
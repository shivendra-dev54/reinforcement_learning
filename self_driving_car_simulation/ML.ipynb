{"cells":[{"cell_type":"code","source":["!pip install gym==0.26.0\n","!pip install numpy==1.24.3\n","!pip install tensorflow==2.12.0\n","!pip install matplotlib==3.7.1\n","!pip install opencv-python==4.7.0.72\n","!pip install pyvirtualdisplay\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"metadata":{"id":"um1o-wmywh5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Self-Driving Car Simulation using Deep Q-Learning\n","# Environment: OpenAI Gym's CarRacing-v2\n","# Optimized for Google Colab\n","\n","import gym\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, Flatten, Dense\n","from tensorflow.keras.optimizers import Adam\n","from collections import deque\n","import cv2\n","import time\n","import os\n","import base64\n","from IPython import display as ipythondisplay\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","from gym.wrappers import RecordVideo\n","import glob\n","\n","# Set up virtual display for rendering\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()\n","\n","# Set random seeds for reproducibility\n","np.random.seed(123)\n","tf.random.set_seed(123)\n","random.seed(123)\n","\n","# Install required packages - uncomment and run in Colab\n","\"\"\"\n","!pip install gym==0.26.0\n","!pip install numpy==1.24.3\n","!pip install tensorflow==2.12.0\n","!pip install matplotlib==3.7.1\n","!pip install opencv-python==4.7.0.72\n","!pip install pyvirtualdisplay\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","\"\"\"\n","\n","# Function to display animation in Colab\n","def show_video(video_path):\n","    \"\"\"\n","    Show a recorded gym video in a Colab notebook\n","    \"\"\"\n","    mp4 = open(video_path, 'rb').read()\n","    encoded = base64.b64encode(mp4).decode('ascii')\n","    display_html = HTML(f\"\"\"\n","    <video width=600 controls>\n","        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n","    </video>\n","    \"\"\")\n","    ipythondisplay.display(display_html)\n","    ipythondisplay.clear_output(wait=True)\n","\n","# Function to show the latest recorded video\n","def show_latest_video():\n","    video_files = sorted(glob.glob('./videos/*.mp4'), key=os.path.getmtime)\n","    if len(video_files) > 0:\n","        show_video(video_files[-1])\n","    else:\n","        print(\"No videos found.\")\n","\n","class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=10000)\n","        self.gamma = 0.95    # discount rate\n","        self.epsilon = 1.0   # exploration rate\n","        self.epsilon_min = 0.1\n","        self.epsilon_decay = 0.9995\n","        self.learning_rate = 0.001\n","        self.update_target_frequency = 1000\n","        self.model = self._build_model()\n","        self.target_model = self._build_model()\n","        self.update_target_model()\n","\n","    def _build_model(self):\n","        # Neural Net for Deep-Q learning Model\n","        model = Sequential()\n","        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=self.state_size))\n","        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n","        model.add(Conv2D(64, (3, 3), activation='relu'))\n","        model.add(Flatten())\n","        model.add(Dense(512, activation='relu'))\n","        model.add(Dense(self.action_size, activation='linear'))\n","        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n","        return model\n","\n","    def update_target_model(self):\n","        # Copy weights from model to target_model\n","        self.target_model.set_weights(self.model.get_weights())\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state, training=True):\n","        if training and np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        act_values = self.model.predict(state, verbose=0)\n","        return np.argmax(act_values[0])  # returns action index\n","\n","    def replay(self, batch_size, step):\n","        if len(self.memory) < batch_size:\n","            return\n","\n","        minibatch = random.sample(self.memory, batch_size)\n","        states, targets = [], []\n","\n","        for state, action, reward, next_state, done in minibatch:\n","            target = self.model.predict(state, verbose=0)[0]\n","            if done:\n","                target[action] = reward\n","            else:\n","                # Use target network for more stable Q-value estimation\n","                t = self.target_model.predict(next_state, verbose=0)[0]\n","                target[action] = reward + self.gamma * np.amax(t)\n","\n","            states.append(state[0])\n","            targets.append(target)\n","\n","        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","        # Update target network periodically\n","        if step % self.update_target_frequency == 0:\n","            self.update_target_model()\n","            print(\"Target model updated!\")\n","\n","    def load(self, name):\n","        self.model.load_weights(name)\n","\n","    def save(self, name):\n","        self.model.save_weights(name)\n","\n","\n","def preprocess_state(state):\n","    \"\"\"\n","    Preprocess the RGB image (96, 96, 3) to grayscale and resize to (84, 84, 1)\n","    \"\"\"\n","    grayscale = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n","    resized = cv2.resize(grayscale, (84, 84), interpolation=cv2.INTER_AREA)\n","    normalized = resized / 255.0\n","    return normalized.reshape(1, 84, 84, 1)\n","\n","def create_discrete_actions():\n","    \"\"\"\n","    Create a set of discrete actions\n","    \"\"\"\n","    # Discrete action space (steering, gas, brake):\n","    # 0: Steer left + gas\n","    # 1: Straight + gas\n","    # 2: Steer right + gas\n","    # 3: Steer left + no gas\n","    # 4: Straight + no gas\n","    # 5: Steer right + no gas\n","    # 6: Brake only\n","    actions = np.array([\n","        [-1.0, 0.5, 0.0],  # Left + gas\n","        [0.0, 0.5, 0.0],   # Straight + gas\n","        [1.0, 0.5, 0.0],   # Right + gas\n","        [-1.0, 0.0, 0.0],  # Left\n","        [0.0, 0.0, 0.0],   # Straight\n","        [1.0, 0.0, 0.0],   # Right\n","        [0.0, 0.0, 0.8],   # Brake\n","    ])\n","    return actions\n","\n","def visualize_agent_state(state, action_idx, discrete_actions, step_reward, total_reward, action_meanings):\n","    \"\"\"\n","    Visualize the current state of the agent and its action\n","    \"\"\"\n","    # Reshape state from (1, 84, 84, 1) to (84, 84)\n","    display_state = state.reshape(84, 84)\n","\n","    plt.figure(figsize=(10, 6))\n","\n","    # Display the state\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(display_state, cmap='gray')\n","    plt.title('Agent View (Grayscale)')\n","    plt.axis('off')\n","\n","    # Display action and reward info\n","    plt.subplot(1, 2, 2)\n","    action = discrete_actions[action_idx]\n","\n","    # Create text for action and reward\n","    info_text = f\"Action: {action_meanings[action_idx]}\\n\"\n","    info_text += f\"Action values: Steer={action[0]:.1f}, Gas={action[1]:.1f}, Brake={action[2]:.1f}\\n\"\n","    info_text += f\"Step Reward: {step_reward:.2f}\\n\"\n","    info_text += f\"Total Reward: {total_reward:.2f}\"\n","\n","    plt.text(0.1, 0.5, info_text, fontsize=12)\n","    plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def train_model(episodes=100, batch_size=32, save_every=10, visualize_every=10, video_every=20):\n","    \"\"\"\n","    Train the DQN agent\n","    \"\"\"\n","    # Create folders\n","    if not os.path.exists('models'):\n","        os.makedirs('models')\n","    if not os.path.exists('videos'):\n","        os.makedirs('videos')\n","\n","    # Define discrete actions\n","    discrete_actions = create_discrete_actions()\n","    num_actions = len(discrete_actions)\n","\n","    # Define action meanings for visualization\n","    action_meanings = [\n","        \"Left + Gas\",\n","        \"Straight + Gas\",\n","        \"Right + Gas\",\n","        \"Left\",\n","        \"Straight\",\n","        \"Right\",\n","        \"Brake\"\n","    ]\n","\n","    # State dimensions (after preprocessing)\n","    state_size = (84, 84, 1)\n","\n","    # Create agent\n","    agent = DQNAgent(state_size, num_actions)\n","\n","    # Keep track of scores\n","    scores = []\n","\n","    for e in range(episodes):\n","        # Create video wrapper if needed\n","        if (e + 1) % video_every == 0:\n","            env = gym.make('CarRacing-v2', render_mode=\"rgb_array\")\n","            env = RecordVideo(env, f'videos/episode_{e+1}',\n","                             episode_trigger=lambda x: True,\n","                             video_length=1000)\n","        else:\n","            env = gym.make('CarRacing-v2', render_mode=\"rgb_array\")\n","\n","        state = env.reset()[0]  # Get initial state\n","        state = preprocess_state(state)\n","\n","        total_reward = 0\n","        done = False\n","        step = 0\n","\n","        while not done:\n","            # Select action\n","            action_idx = agent.act(state)\n","            action = discrete_actions[action_idx]\n","\n","            # Take action\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","\n","            # Preprocess next state\n","            next_state = preprocess_state(next_state)\n","\n","            # Modify reward to encourage staying on track\n","            if reward < 0:\n","                reward *= 5  # Penalize going off track more heavily\n","\n","            total_reward += reward\n","\n","            # Remember experience\n","            agent.remember(state, action_idx, reward, next_state, done)\n","\n","            # Visualize state and action periodically\n","            if (e + 1) % visualize_every == 0 and step % 20 == 0:\n","                visualize_agent_state(state, action_idx, discrete_actions, reward, total_reward, action_meanings)\n","\n","            # Train agent\n","            agent.replay(batch_size, step)\n","\n","            # Update state\n","            state = next_state\n","            step += 1\n","\n","            # Limit maximum steps per episode\n","            if step > 1000:\n","                break\n","\n","        # Close environment\n","        env.close()\n","\n","        # Save score\n","        scores.append(total_reward)\n","\n","        # Print progress\n","        print(f\"Episode: {e+1}/{episodes}, Score: {total_reward:.2f}, Epsilon: {agent.epsilon:.2f}\")\n","\n","        # Save model periodically\n","        if (e + 1) % save_every == 0:\n","            agent.save(f\"models/car_racing_dqn_{e+1}.h5\")\n","\n","            # Plot scores\n","            plt.figure(figsize=(10, 5))\n","            plt.plot(scores)\n","            plt.title('Training Progress')\n","            plt.xlabel('Episode')\n","            plt.ylabel('Score')\n","            plt.savefig(f'training_progress_{e+1}.png')\n","            plt.show()\n","\n","        # Display the recorded video\n","        if (e + 1) % video_every == 0:\n","            print(\"Displaying the latest recorded episode:\")\n","            show_latest_video()\n","\n","    # Final save\n","    agent.save(\"models/car_racing_dqn_final.h5\")\n","\n","    # Plot final scores\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(scores)\n","    plt.title('Training Progress')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Score')\n","    plt.savefig('training_progress_final.png')\n","    plt.show()\n","\n","    print(\"Training complete!\")\n","    return scores, agent\n","\n","\n","def test_agent(model_path, num_episodes=3):\n","    \"\"\"\n","    Test a trained agent and display video\n","    \"\"\"\n","    # Create environment\n","    env = gym.make('CarRacing-v2', render_mode=\"rgb_array\")\n","    env = RecordVideo(env, 'videos/test', episode_trigger=lambda x: True)\n","\n","    # Define discrete actions\n","    discrete_actions = create_discrete_actions()\n","    num_actions = len(discrete_actions)\n","\n","    # State dimensions\n","    state_size = (84, 84, 1)\n","\n","    # Create agent\n","    agent = DQNAgent(state_size, num_actions)\n","\n","    # Load trained weights\n","    agent.load(model_path)\n","    agent.epsilon = 0.01  # Small epsilon for minimal exploration\n","\n","    for episode in range(num_episodes):\n","        state = env.reset()[0]\n","        state = preprocess_state(state)\n","\n","        total_reward = 0\n","        done = False\n","        step = 0\n","\n","        while not done:\n","            # Select action\n","            action_idx = agent.act(state, training=False)\n","            action = discrete_actions[action_idx]\n","\n","            # Take action\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","\n","            # Preprocess next state\n","            next_state = preprocess_state(next_state)\n","\n","            total_reward += reward\n","            state = next_state\n","            step += 1\n","\n","            # Limit steps\n","            if step > 1000:\n","                break\n","\n","        print(f\"Test Episode: {episode+1}/{num_episodes}, Score: {total_reward:.2f}\")\n","\n","    env.close()\n","\n","    # Show the recorded test video\n","    print(\"Displaying the test video:\")\n","    show_latest_video()\n","\n","\n","if __name__ == \"__main__\":\n","    # Train agent\n","    print(\"Starting training...\")\n","    scores, agent = train_model(episodes=100, batch_size=32, save_every=10, visualize_every=5, video_every=20)\n","\n","    # Test trained agent\n","    print(\"Testing the trained agent...\")\n","    test_agent(\"models/car_racing_dqn_final.h5\")\n","\n","# Example additional cell for Colab notebook to evaluate model performance\n","\"\"\"\n","# Plot learning curve\n","plt.figure(figsize=(10, 5))\n","plt.plot(scores)\n","plt.title('Training Progress')\n","plt.xlabel('Episode')\n","plt.ylabel('Score')\n","plt.grid(True)\n","plt.show()\n","\n","# Show a recorded video of the agent's performance\n","show_latest_video()\n","\"\"\""],"metadata":{"id":"7IS7KpSAuzZN"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"15kHHOGDIlWoomsF6Z2TwRc3BDBIwFLQ0","authorship_tag":"ABX9TyMALoAtBoS1Y48SICie72CG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}